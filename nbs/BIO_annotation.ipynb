{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for tokenising with space as token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize text\n",
    "\n",
    "def tokenize(raw):\n",
    "    doc = nlp(raw)\n",
    "    token_texts = []\n",
    "    for token in doc:\n",
    "        token_texts.append(token.text)\n",
    "        if token.whitespace_:  # filter out empty strings\n",
    "            token_texts.append(token.whitespace_)\n",
    "    return token_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illustrative example on English Legal Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_json('C:/Users/sadan/OneDrive/Surrey/acronyms/PR-AAAI22-SDU-ST1-AE/data/english/legal/train.json', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokenized'] =data['text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>acronyms</th>\n",
       "      <th>longforms</th>\n",
       "      <th>ID</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12).;  Terms of reference A Correspondence Gro...</td>\n",
       "      <td>[[194, 199]]</td>\n",
       "      <td>[[164, 192]]</td>\n",
       "      <td>1</td>\n",
       "      <td>[12, ), ., ;,  ,  , Terms,  , of,  , reference...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The comprehensive list of currently identifie...</td>\n",
       "      <td>[[233, 238]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>[ , The,  , comprehensive,  , list,  , of,  , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subregional activities for development Legisl...</td>\n",
       "      <td>[[142, 147]]</td>\n",
       "      <td>[[85, 140]]</td>\n",
       "      <td>3</td>\n",
       "      <td>[ , Subregional,  , activities,  , for,  , dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OIOS recommended that Secretariat programmes t...</td>\n",
       "      <td>[[239, 247], [142, 146], [0, 4]]</td>\n",
       "      <td>[[167, 237]]</td>\n",
       "      <td>4</td>\n",
       "      <td>[OIOS,  , recommended,  , that,  , Secretariat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98. The Ministry of Education and Culture has...</td>\n",
       "      <td>[[82, 86]]</td>\n",
       "      <td>[[71, 80]]</td>\n",
       "      <td>5</td>\n",
       "      <td>[ , 98, .,  , The,  , Ministry,  , of,  , Educ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  12).;  Terms of reference A Correspondence Gro...   \n",
       "1   The comprehensive list of currently identifie...   \n",
       "2   Subregional activities for development Legisl...   \n",
       "3  OIOS recommended that Secretariat programmes t...   \n",
       "4   98. The Ministry of Education and Culture has...   \n",
       "\n",
       "                           acronyms     longforms  ID  \\\n",
       "0                      [[194, 199]]  [[164, 192]]   1   \n",
       "1                      [[233, 238]]            []   2   \n",
       "2                      [[142, 147]]   [[85, 140]]   3   \n",
       "3  [[239, 247], [142, 146], [0, 4]]  [[167, 237]]   4   \n",
       "4                        [[82, 86]]    [[71, 80]]   5   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [12, ), ., ;,  ,  , Terms,  , of,  , reference...  \n",
       "1  [ , The,  , comprehensive,  , list,  , of,  , ...  \n",
       "2  [ , Subregional,  , activities,  , for,  , dev...  \n",
       "3  [OIOS,  , recommended,  , that,  , Secretariat...  \n",
       "4  [ , 98, .,  , The,  , Ministry,  , of,  , Educ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the name of long-forms column for applying the function directly on the column\n",
    "data = data.rename(columns={\"long-forms\": \"longforms\"})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  The function for annotating with BIO:\n",
    " 1. Long-forms are annotated as: B-LF, I-LF\n",
    " 2. Acronyms are annotated as: B-AC (all subwords are concatenated e.g. un-women are treated as one acronym)\n",
    " 3. Other is annotated as: B-O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio(text,longforms,acronyms,tokens):\n",
    "    \n",
    "    acr_acc = ''\n",
    "    acr_start_flag = 0\n",
    "    char_index = 0\n",
    "    label_flag = 0\n",
    "    sent_labeled = []\n",
    "    for w in tokens:\n",
    "        label_flag = 0\n",
    "        if w in [' ', '(', ')'] and not(acr_start_flag):\n",
    "            char_index += 1\n",
    "            continue  \n",
    "        #check longforms\n",
    "        for indx in longforms:\n",
    "            if char_index == indx[0]:\n",
    "                sent_labeled.append((w,'B-LF'))\n",
    "                label_flag = 1\n",
    "                break\n",
    "            elif indx[0] < char_index < indx[1]:\n",
    "                sent_labeled.append((w,'I-LF'))\n",
    "                label_flag = 1\n",
    "                break\n",
    "        \n",
    "        # check acronym\n",
    "        for indx in acronyms:\n",
    "            \n",
    "            if acr_start_flag and char_index >= indx[1]:\n",
    "               \n",
    "                sent_labeled.append((acr_acc,'B-AC'))\n",
    "                acr_start_flag = 0\n",
    "                acr_acc = ''\n",
    "            if char_index == indx[0]:\n",
    "                \n",
    "                acr_acc += w\n",
    "                label_flag = 1\n",
    "                acr_start_flag = 1\n",
    "                break\n",
    "            elif indx[0] < char_index < indx[1]:\n",
    "                \n",
    "                acr_acc += w\n",
    "                \n",
    "                label_flag = 1\n",
    "                break\n",
    "            #  check O label\n",
    "        if label_flag == 0:\n",
    "            sent_labeled.append((w,'B-O'))\n",
    "        char_index += len(w)\n",
    "        \n",
    "    return sent_labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the function to the dataframe directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_eng_train_legal = data.apply(lambda x: bio(x.text, x.longforms,x.acronyms,x.tokenized), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the BIO list into a txt file with each token, BIO annotation on one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bio_eng_leg_train.txt', 'w') as outfile:\n",
    "    for ls in bio_eng_train_legal:\n",
    "        for tup in ls:\n",
    "            line = \" \".join(map(str, tup))\n",
    "        \n",
    "            outfile.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nm]",
   "language": "python",
   "name": "conda-env-nm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
